0701 - D   -- DONE
0702 - F   -- DONE
0703 - D   -- DONE
0704a - F  -- DONE
0704b - D  -- DONE
0705a - D  -- DONE
0705b - D  -- DONE
0705c - D  -- DONE
0705d - D  -- DONE
0706 - F   -- DONE
0707 - F   -- DONE
0708a - F  -- DONE
0708b - F  -- DONE

0801 - F
0802 - D   -- DONE
0803 - F
0804 - D   -- DONE
0805 - F
0806 - D   -- DONE
0807 - F
0808 - D   -- DONE
0809 - F

#Per forzare l'uso della CPU
import os
os.environ['CUDA_VISIBLE_DEVICES'] = ''



Indagini messaggio di warning iniziale


Modifiche apportate


main references:

R1. Sessions are no longer supported as of TF 2.0, as well as placeholder and all sessions' related features. Indeed, TF 2.0 "prefers functions over sessions and integrates better with the Python runtime with Eager execution enabled by default along with tf.function that provides automatic control dependencies for graphs and compilation." (https://www.tensorflow.org/guide/migrate/tf1_vs_tf2)


main changes across most notebooks:

(1.) Removed placeholder (R1) and training dataset type modified manually with ".astype('float32')" function.
(2.) model and loss each put into separated functions (R1).
(3.) training made into a train_step function decorated with @tf.function (to produce and be executed as a graph, just as the training session was in TF 1.0) nested within a loop over epochs. GradientTape() function used to track and better compute derivates along the generated graph.
(4.) optimizers functions adapted to TF 2.0 versions: 
from tf.train.GradientDescentOptimizer to tf.optimizers.SGD
from tf.train.MomentumOptimizer to tf.optimizers.SGD (momentum version embedded and activated by arguments)
from tf.train.AdamOptimizer to tf.optimizers.Adam



#Colab 0701:

- Change of the imports adaptation to TF 2.0
- Remove references to the Session not handled anymore in TF 2.0
- Resolution of not working operations among tensors using the module linAlg compatible to TF 2.16:
  sobstitution of function calls in accordance to Api_Docs: https://www.tensorflow.org/api_docs/python/tf/linalg/
- Remove of global_variables_initializer not used in TF2: variables are initialized immediately when they are created. 
  here is no longer a need to run variable initializers before using them.
- Change of the function to random distribution from tf.random_uniform a tf.random.uniform
- Pandas installation to make work a subexample that is commented: pip install pandas

#Colab 0702:
- (1.)
- (2.)
- (3.)
- (4.)
- testing was put into separated function (R1).

#Colab 0703:
- Change of the imports adaptation to TF 2.0
- Forced the reshape of the training dataset to float32 
- Remove handling of placeholder
- Ridefinition of the function to model definition
- Ridefinition of function loss, adaptation from  tf.nn.softmax_cross_entropy_with_logits_v2 to tf.nn.softmax_cross_entropy_with_logits
- Change definition and handling of the optimizer, tf.train.GradientDescentOptimizer -> tf.optimizers.SGD
- Ridefinition of training function step by step, training procedure

#Colab 0704a:
- Change MNIST dataset import, not from tensorflow.examples but from tensorflow.keras.
- Due to different import location, training dataset needed to undergo Min-Max-Scalarization, reshape from (60000,28,28,1) to (60000,28*28) and numerical to categorcal labels trasformation. similarly for the test dataset.
- Changed way to do logs accordingly to tf.logging package new usage
- (1.)
- (2.)
- (3.)
- (4.)
- (5.)

#Colab 0704b:
- Change import dataset MNIST, not yet from tensorflow.keras but from keras.datasets
- Change way to do logs, import separately
- Adapt loading dataset and handling of training and test set
- Sobstitution of tf.random_normal to tf.random.normal
- Change Optimizer into Adam


#Colab 0705a:
-Change of the imports adaptation to TF 2.0
- Adapted TensorBoard to TF 2.0  using @tf.function and producing 2 distinct tensor graphs.


#Colab 0705b: 
- Change of the imports adaptation to TF 2.0
- Adjusted unworking commands to execute a scalar summary in the tf log file
- Give the possibility to run TensorBoard from  Jupyter Notebook


#Colab 0705c:
- Change of the imports adaptation to TF 2.0
- Removed unsed tf.reset_default_graph() because TF2.0 works in eager mode
- Ridefinition of a module using @tf.function
- Calling in the proper way the method for printing the result into TensorBoard log

#Colab 0705d:
- Change of the imports adaptation to TF 2.0
- Ridefinition of a module using @tf.function
- Calling in the proper way the method for printing the result and graph into TensorBoard log

# Colab 0706:
- Change MNIST dataset import, not from tensorflow.examples but from tensorflow.keras.
- Due to different import location, training dataset needed to undergo Min-Max-Scalarization, dimension expansion and numerical to categorcal labels trasformation. similarly for the test dataset.
- Changed way to do logs accordingly to tf.logging package new usage (imported separatedly)
- (1.)
- (2.)
- (3.)
- (4.)
- (5.)
- various functions adapted to TF 2.0 versions:
from tf.random_normal to tf.random.normal

# Colab 0707:
- Change cifar10 dataset import, not from tensorflow.keras.dataset but from keras.dataset.
- (1.)
- (2.) 
- Following TF 2.0 logic, all CNN layers were defined within model function itself. 
- (3.)
- (4.)
- various functions adapted to TF 2.0 versions:
from tf.random_normal to tf.random.normal

# Colab 0708a:
- (1.)
- (2.)
- Model function (of the autoencoder) required to be splitted into 2 further functions, the encoder and decoder functions, in such a way that the single encoder function could be called separatedly to provide the hidden layer representation of the input object.
- (3.)
- During training no further sessions was needed to compute MSE, but a simple return of the loss of the train_step() function.


# Colab 0708b:
- Change MNIST dataset import, not from tensorflow.examples but from tensorflow.keras.
- Due to different import location, dataset needed to undergo Min-Max-Scalarization and dimension expansion.
- (1.)
- Instead of building the model by mean of functions, a class implementing tf.Module was defined. This was made because tf.saved_model.save supports saving tf.Module and its subclasses. Then all weights and baises were defined as instance variables of the Model class, as well as encoder and decoder functions were defined as class functions. Finally, the actual model implementation was defined within __call__() function of the same class. All class functions (except __init__()) were also decorated with @tf.function since only tf.Variable attributes, tf.function-decorated methods, and tf.Modules found via recursive traversal are saved by tf.saved_model.save.
- (3.)
- During training no further sessions was needed to compute MSE, but a simple return of the loss of the train_step() function.
- to save the model, functions compatible to TF 2.0 (tf.saved_model.save and tf.saved_model.load) were used instead of the no more supported ones.

#Colab 0801
- Adapted imports from directly keras
- Fixing working Adam parameter learning rate

#Colab 0802
- Adapted imports from directly keras
- Fixing working Adam parameter learning rate

#Colab 0803
- Adapted imports from directly keras
- Fixing working Adam parameter learning rate

#Colab 0804
- Adapted imports from directly keras
- Fixing working Adam parameter learning rate

#Colab 0805
- Request to install the following packages: 
  pip install statsmodels

- Adapted imports from directly keras
- Fixing working Adam parameter learning rate
- model.__call__() used instead of model.predict() because, in the latest version of keras, the latter is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time.

#Colab 0806
- Adapted imports from directly keras
- Fixing working Adam parameter learning rate

#Colab 0807
- Adapted imports from directly keras
- Fixing working Adam parameter learning rate

#Colab 0808
- Adapted imports from directly keras
- Fixing working Adam parameter learning rate

#Colab 0809
- Request to install the following packages: 
  pip install gensim
  pip install scipy==1.10.1 -- Previous version because the version above give errors
  pip install beautifulsoup4
  pip install lxml
  pip install nltk

- Force downloads of two package inside nltk to not have errors during pre_processing:
  nltk.download('punkt')
  nltk.download('stopwords')

- Adapted Word2Vec initialization to the new gensim version >=4.0 parameter vector_size
- Change the way to return the vocabulary according to new version >=4.0 -> my_model.wv.key_to_index









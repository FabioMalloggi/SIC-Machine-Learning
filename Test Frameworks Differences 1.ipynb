{"cells":[{"cell_type":"markdown","metadata":{"id":"l-8BUvC5sP3C"},"source":["# Malloggi-Vigna SEAI Project - Project Test Base\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":244,"status":"ok","timestamp":1717689166922,"user":{"displayName":"Fabio M","userId":"09614366475601969276"},"user_tz":-120},"id":"Wma6P1HzxARc","outputId":"7ede70b7-75ee-4613-91c2-9ed0fd24f002"},"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2023 NVIDIA Corporation\n","Built on Tue_Aug_15_22:02:13_PDT_2023\n","Cuda compilation tools, release 12.2, V12.2.140\n","Build cuda_12.2.r12.2/compiler.33191640_0\n"]}],"source":["!nvcc  --version"]},{"cell_type":"markdown","metadata":{"id":"uDdeEXXYGpjn","jp-MarkdownHeadingCollapsed":true},"source":["## Colab (ONLY) Environment Setup"]},{"cell_type":"markdown","metadata":{"id":"7e8E0JtFeD51"},"source":["reference:\n","https://keras.io/getting_started/\n","\n","Aim of this project:\n","- Keras 3\n","- Tensorflow 2.16.1 (only one compatible with Keras3)\n","\n","Colab preinstalled packages:\n","- Keras 2\n","- Tensorflow 2.15.0\n","- tf-keras 2.15.1 (previous keras version which was defined as tensorflow sub-package)\n","\n","Therefore we need to update Tensorflow to the latest version available.\n","\n","\n","---\n","reference: https://github.com/keras-team/tf-keras\n","\n","TF-Keras: the pure-TensorFlow implementation of Keras\n","This repository hosts the development of the TF-Keras library. It is a pure TensorFlow implementation of Keras, based on the legacy tf.keras codebase.\n","\n","Note that the \"main\" version of Keras is now Keras 3 (formerly Keras Core), which is a multi-backend implementation of Keras, supporting JAX, PyTorch, and TensorFlow. Keras 3 is being developed at keras-team/keras."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":84525,"status":"ok","timestamp":1717689254911,"user":{"displayName":"Fabio M","userId":"09614366475601969276"},"user_tz":-120},"id":"lrGR7Un46UM_","outputId":"a0d06f45-7747-40e3-aed0-2b35aa50b6c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: tensorflow 2.15.0\n","Uninstalling tensorflow-2.15.0:\n","  Successfully uninstalled tensorflow-2.15.0\n","Found existing installation: keras 2.15.0\n","Uninstalling keras-2.15.0:\n","  Successfully uninstalled keras-2.15.0\n","Found existing installation: tf_keras 2.15.1\n","Uninstalling tf_keras-2.15.1:\n","  Successfully uninstalled tf_keras-2.15.1\n","Collecting tensorflow\n","  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Collecting h5py>=3.10.0 (from tensorflow)\n","  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n","Collecting ml-dtypes~=0.3.1 (from tensorflow)\n","  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.31.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.1)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n","Collecting tensorboard<2.17,>=2.16 (from tensorflow)\n","  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting keras>=3.0.0 (from tensorflow)\n","  Downloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n","Collecting namex (from keras>=3.0.0->tensorflow)\n","  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n","Collecting optree (from keras>=3.0.0->tensorflow)\n","  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.6.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (2.16.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n","Installing collected packages: namex, optree, ml-dtypes, h5py, tensorboard, keras, tensorflow\n","  Attempting uninstall: ml-dtypes\n","    Found existing installation: ml-dtypes 0.2.0\n","    Uninstalling ml-dtypes-0.2.0:\n","      Successfully uninstalled ml-dtypes-0.2.0\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.9.0\n","    Uninstalling h5py-3.9.0:\n","      Successfully uninstalled h5py-3.9.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.15.2\n","    Uninstalling tensorboard-2.15.2:\n","      Successfully uninstalled tensorboard-2.15.2\n","Successfully installed h5py-3.11.0 keras-3.3.3 ml-dtypes-0.3.2 namex-0.0.8 optree-0.11.0 tensorboard-2.16.2 tensorflow-2.16.1\n"]}],"source":["# tensorflow 2.15.0 is already installed, it must be uninstalled first\n","!pip uninstall tensorflow -y\n","!pip uninstall keras -y\n","!pip uninstall tf-keras -y\n","!pip install tensorflow\n","# !pip install --upgrade keras //automatically done installing tensorflow"]},{"cell_type":"markdown","metadata":{"id":"fZRdocSCu79b"},"source":["# KERAS 3 - Multi-backend High-level API\n","Keras is the high-level API of the TensorFlow platform. It provides an approachable, highly-productive interface for solving machine learning (ML) problems, with a focus on modern deep learning."]},{"cell_type":"markdown","metadata":{"id":"GVGrmCiFeD52"},"source":["## Backend instantiation\n","Remember to configure the backend before importing Keras (and the other modules), as the backend cannot be changed once the package is imported."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z1IB3HFVrdpC"},"outputs":[],"source":["# Available backend options are: \"jax\", \"tensorflow\", \"torch\".\n","import os\n","os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""]},{"cell_type":"markdown","metadata":{"id":"6S1xZiAueD52"},"source":["## frameworks version"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18347,"status":"ok","timestamp":1717689280344,"user":{"displayName":"Fabio M","userId":"09614366475601969276"},"user_tz":-120},"id":"3YU8vgnahsEd","outputId":"0f8a0529-0438-4ac6-ba59-2298d28c36d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["python3: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n","tensorflow: 2.16.1\n","keras: 3.3.3\n","numpy: 1.25.2\n","torch: 2.3.0+cu121\n","torchvision: 0.18.0+cu121\n","CUDA: 12.1\n","cuDNN: 8.9.6\n"]}],"source":["import sys\n","import tensorflow as tf\n","import jax\n","import jaxlib\n","import keras\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms\n","from jax import numpy as jnp\n","\n","print(f\"python3: {sys.version}\")\n","print(f\"tensorflow: {tf.__version__}\")\n","print(f\"jax: {jax.__version__}\")\n","print(f\"keras: {keras.__version__}\")\n","print(f\"numpy: {np.__version__}\")\n","print(f\"torch: {torch.__version__}\")\n","print(f\"torchvision: {torchvision.__version__}\")\n","print(f\"CUDA: {torch.version.cuda}\")\n","\n","cudnn = torch.backends.cudnn.version()\n","cudnn_major = cudnn // 1000\n","cudnn = cudnn % 1000\n","cudnn_minor = cudnn // 100\n","cudnn_patch = cudnn % 100\n","print( 'cuDNN:', '.'.join([str(cudnn_major),str(cudnn_minor),str(cudnn_patch)]) )"]},{"cell_type":"markdown","metadata":{"id":"S6KNFNVHeD52"},"source":["## GPU support"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BbUt693AeD52","outputId":"640a3c6e-5fe3-437e-f1da-e815f381739e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716473587624,"user_tz":-120,"elapsed":8,"user":{"displayName":"Fabio M","userId":"09614366475601969276"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[]\n"]}],"source":["from tensorflow.python.client import device_lib\n","\n","# Hide GPU from visible devices (force CPU-ONLY processing)\n","#os.environ['CUDA_VISIBLE_DEVICES'] = '' OR #tf.config.set_visible_devices([], 'GPU')\n","\n","gpus = tf.config.list_physical_devices('GPU')\n","print([d.name for d in gpus])\n","\n","# do not uncomment, check https://github.com/tensorflow/tensorflow/issues/9374\n","# print(device_lib.list_local_devices())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"thE2mCDFeD52","outputId":"47328ae4-f262-4724-c938-0dc3e4b02e68","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716473587624,"user_tz":-120,"elapsed":8,"user":{"displayName":"Fabio M","userId":"09614366475601969276"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]}],"source":["# setting device on GPU if available, else CPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)\n","\n","#Additional Info when using cuda\n","if device.type == 'cuda':\n","    print(torch.cuda.get_device_name(0))\n","    print('Currently Memory Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n","    print('Currently Memory Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"]},{"cell_type":"markdown","metadata":{"id":"8CMV1R6P7-jc"},"source":["# Code To Test: LeNet-5 Architecture (CNN)\n","\n","\n","---\n","reference:\n","\n","pytorch & tensorflow 2.15 (compatible with keras2) examples: https://towardsdatascience.com/tensorflow-vs-pytorch-convolutional-neural-networks-cnn-dd9ca6ddafce\n","\n","pytorch (compatible with keras3) example: https://medium.com/@shivansh.kaushik/keras-core-3-0-the-multi-backend-beast-ac46609adb98\n","\n","keras example: https://keras.io/examples/vision/mnist_convnet/\n"]},{"cell_type":"markdown","metadata":{"id":"ZKDlVvA_Ns2B","jp-MarkdownHeadingCollapsed":true},"source":["## PyTorch LeNet-5 Architecture (CNN)"]},{"cell_type":"markdown","metadata":{"id":"myi6iTGDeD53"},"source":["Step 0: Parameters setting and Libraries import"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mwttHqIheD53"},"outputs":[],"source":["batch_size_pt = 60000\n","epochs_pt = 10"]},{"cell_type":"markdown","metadata":{"id":"-qf8dLlAzP5x"},"source":["Step 1: Getting, Splitting and Loading the Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qf4JBdjAzPl5"},"outputs":[],"source":["#PyTorch - Getting and Splitting the Dataset\n","transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n","train_dataset_pt = torchvision.datasets.FashionMNIST(root='./data/',\n","                                                     train=True,\n","                                                     transform=transforms,\n","                                                     download=True)\n","test_dataset_pt = torchvision.datasets.FashionMNIST(root='.data/',\n","                                                     train=False,\n","                                                     transform=transforms,\n","                                                     download=True)\n","\n","#PyTorch - Loading the Data\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset_pt,\n","                                           batch_size=batch_size_pt,\n","                                           shuffle=False)\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset_pt,\n","                                           #batch_size=32,\n","                                           shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"0P4lPfykeD53"},"source":["Step 2: Preparing the Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E83j5KpneD53"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"qG8qJD8kzvoS"},"source":["Step 3: Building the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1716473587624,"user":{"displayName":"Fabio M","userId":"09614366475601969276"},"user_tz":-120},"id":"e3uTv2REzu6y","outputId":"2c34a1db-2891-4cab-c3bb-1a03169bca30","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sequential(\n","  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","  (1): ReLU()\n","  (2): AvgPool2d(kernel_size=(2, 2), stride=2, padding=0)\n","  (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n","  (4): ReLU()\n","  (5): AvgPool2d(kernel_size=(2, 2), stride=2, padding=0)\n","  (6): Flatten(start_dim=1, end_dim=-1)\n","  (7): Linear(in_features=400, out_features=120, bias=True)\n","  (8): ReLU()\n","  (9): Linear(in_features=120, out_features=84, bias=True)\n","  (10): ReLU()\n","  (11): Linear(in_features=84, out_features=10, bias=True)\n","  (12): Softmax(dim=1)\n",")"]},"metadata":{},"execution_count":19}],"source":["#PyTorch - Building the Model\n","model_pt = nn.Sequential(\n","    # Layer1 = Conv1\n","    nn.Conv2d(1, 6, (5, 5), stride=1, padding=2),\n","    nn.ReLU(),\n","    nn.AvgPool2d((2, 2), stride=2),\n","\n","    # Layer2 = Conv2\n","    nn.Conv2d(6, 16, (5, 5), stride=1, padding=0),\n","    nn.ReLU(),\n","    nn.AvgPool2d((2, 2), stride=2),\n","\n","    nn.Flatten(),\n","    # Layer3 = Conv3\n","    nn.Linear(400,120),\n","    nn.ReLU(),\n","    nn.Linear(120,84),\n","    nn.ReLU(),\n","    nn.Linear(84,10),\n","    nn.Softmax(dim=1)\n",")\n","model_pt.to(device)"]},{"cell_type":"markdown","metadata":{"id":"WWEUNjLJ0Tdp"},"source":["Step 4: Training the Model"]},{"cell_type":"markdown","metadata":{"id":"pU91JzmGeD54"},"source":["reference:\n","https://medium.com/@soumensardarintmain/manage-cuda-cores-ultimate-memory-management-strategy-with-pytorch-2bed30cab1#:~:text=The%20recommended%20way%20is%20to,first%20and%20then%20call%20torch.\n","\n","---\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cbLxC3JH0WSI","outputId":"c334a439-f82d-46c9-a288-6828ec82db7b"},"outputs":[{"output_type":"stream","name":"stderr","text":["Exception ignored in: <function _xla_gc_callback at 0x79af373deb90>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\", line 98, in _xla_gc_callback\n","    def _xla_gc_callback(*args):\n","KeyboardInterrupt: \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0 \t Time: 44.413 sec \t Loss: 2.303\n"]}],"source":["import time\n","import gc\n","\n","criterion = nn.CrossEntropyLoss()\n","optim = torch.optim.Adam(model_pt.parameters())\n","\n","times = []\n","\n","#PyTorch - Training the Model\n","for e in range(epochs_pt):\n","    start_epoch = time.time()\n","\n","    # define the loss value after the epoch\n","    losss = 0.0\n","    number_of_sub_epoch = 0\n","\n","    # loop for every training batch (one epoch)\n","    for images, labels in train_loader:\n","\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        #create the output from the network\n","        out = model_pt(images)\n","        # count the loss function\n","        loss = criterion(out, labels)\n","        # in pytorch you have assign the zero for gradien in any sub epoch\n","        optim.zero_grad()\n","        # count the backpropagation\n","        loss.backward()\n","        # learning\n","        optim.step()\n","        # add new value to the main loss\n","        losss += loss.item()\n","        number_of_sub_epoch += 1\n","\n","\n","    #torch.cuda.synchronize()\n","    end_epoch = time.time()\n","    print(\"Epoch {} \\t Time: {:.3f} sec \\t Loss: {:.3f}\".format(e, (end_epoch-start_epoch), losss / number_of_sub_epoch))\n","    elapsed = end_epoch - start_epoch\n","    times.append(elapsed)\n","\n","    #Cache Cleaning\n","    #The recommended way is to delete the local variables (using del) first\n","    del images\n","    del labels\n","    del losss\n","    del out\n","    # Then clean the cache\n","    torch.cuda.empty_cache()\n","    # then collect the garbage\n","    gc.collect()\n","\n","tot_time_pt = sum(times)\n","avg_time_pt = sum(times)/epochs_pt\n","print(\"Backend: {} \\t Framework: {}\".format(os.environ[\"KERAS_BACKEND\"], \"pytorch\"))\n","print(\"batch size: {:.3f}\".format(batch_size_pt))\n","print(\"total time: {:.3f}\".format(tot_time_pt))\n","print(\"average time: {:.3f}\".format(avg_time_pt))"]},{"cell_type":"markdown","metadata":{"id":"GLDTaUMG0ihp"},"source":["Step 5: Evaluating the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MMZk1bBg0op5"},"outputs":[],"source":["#PyTorch - Comparing the Results\n","correct = 0\n","total = 0\n","model_pt.eval()\n","for images, labels in test_loader:\n","    images = images.to(device)\n","    labels = labels.to(device)\n","    outputs = model_pt(images)\n","    _, predicted = torch.max(outputs.data, 1)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum()\n","print('Test Accuracy of the model on the {} test images: {}% with PyTorch'.format(total, 100 * correct // total))"]},{"cell_type":"markdown","metadata":{"id":"vMhRYToVy66R","jp-MarkdownHeadingCollapsed":true},"source":["## Keras-3 LeNet-5 Architecture (CNN)"]},{"cell_type":"markdown","metadata":{"id":"i2ZyMKEUzWxB"},"source":["Step 1: Getting, Splitting and Loading the Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZWNbuGCgzWGp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717667443899,"user_tz":-120,"elapsed":1105,"user":{"displayName":"Fabio M","userId":"09614366475601969276"}},"outputId":"5e4def7b-4d3a-4101-ad08-3c802ae13839"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","29515/29515 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","26421880/26421880 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","5148/5148 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","4422102/4422102 [==============================] - 0s 0us/step\n"]}],"source":["# Model / data parameters\n","num_classes = 10\n","input_shape = (28, 28, 1)\n","#TensorFlow - Getting and Splitting the Dataset\n","fashion_mnist = keras.datasets.fashion_mnist\n","#TensorFlow - Loading the Data\n","(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"]},{"cell_type":"markdown","metadata":{"id":"LPk-ooZTeD55"},"source":["Step 2: Prepare the Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DboAEB4AeD55","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717667446541,"user_tz":-120,"elapsed":2,"user":{"displayName":"Fabio M","userId":"09614366475601969276"}},"outputId":"d5353446-1f1c-4e63-8e37-238530d84622"},"outputs":[{"output_type":"stream","name":"stdout","text":["x_train shape: (60000, 28, 28, 1)\n","60000 train samples\n","10000 test samples\n"]}],"source":["# Scale images to the [0, 1] range\n","x_train = x_train.astype(\"float32\") / 255\n","x_test = x_test.astype(\"float32\") / 255\n","\n","# Make sure images have shape (28, 28, 1)\n","x_train = np.expand_dims(x_train, -1)\n","x_test = np.expand_dims(x_test, -1)\n","\n","print(\"x_train shape:\", x_train.shape)\n","print(x_train.shape[0], \"train samples\")\n","print(x_test.shape[0], \"test samples\")\n","\n","# convert class vectors to binary class matrices\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)"]},{"cell_type":"markdown","metadata":{"id":"S-0SdrkJzwmx"},"source":["Step 3: Building the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qg0scwEH0EI5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717667450272,"user_tz":-120,"elapsed":546,"user":{"displayName":"Fabio M","userId":"09614366475601969276"}},"outputId":"1f6cb9d4-8732-416d-90be-4168a436b577"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 28, 28, 6)         156       \n","                                                                 \n"," average_pooling2d (Average  (None, 14, 14, 6)         0         \n"," Pooling2D)                                                      \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 14, 14, 16)        2416      \n","                                                                 \n"," average_pooling2d_1 (Avera  (None, 7, 7, 16)          0         \n"," gePooling2D)                                                    \n","                                                                 \n"," flatten (Flatten)           (None, 784)               0         \n","                                                                 \n"," dense (Dense)               (None, 120)               94200     \n","                                                                 \n"," dense_1 (Dense)             (None, 84)                10164     \n","                                                                 \n"," dense_2 (Dense)             (None, 10)                850       \n","                                                                 \n","=================================================================\n","Total params: 107786 (421.04 KB)\n","Trainable params: 107786 (421.04 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["#Keras - Building the Model\n","model_ks = keras.Sequential(\n","    [\n","        keras.Input(shape=input_shape),\n","        keras.layers.Conv2D(6, kernel_size=(5, 5), strides=1, padding=\"same\", activation=\"relu\"),\n","        keras.layers.AveragePooling2D(pool_size=(2, 2), strides=2),\n","        keras.layers.Conv2D(16, kernel_size=(5, 5), strides=1, padding=\"same\", activation=\"relu\"),\n","        keras.layers.AveragePooling2D(pool_size=(2, 2), strides=2),\n","        keras.layers.Flatten(),\n","        keras.layers.Dense(120, activation=\"relu\"),\n","        keras.layers.Dense(84, activation=\"relu\"),\n","        keras.layers.Dense(num_classes, activation=\"softmax\")\n","    ]\n",")\n","#Keras - Visualizing the Model\n","model_ks.summary()"]},{"cell_type":"markdown","metadata":{"id":"tMmoUROP0Sng"},"source":["Step 4: Training the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yxoKsphY0fNx"},"outputs":[],"source":["#Keras - Training the Model\n","model_ks.compile(loss=\"categorical_crossentropy\",\n","                    optimizer=\"adam\",\n","                    metrics=[\"accuracy\"])\n","\n","batch_size_ks = 128\n","validation_split_ks = 0.1\n","epochs_ks = 10\n","\n","model_ks.fit(x_train, y_train, batch_size=batch_size_ks, epochs=epochs_ks, validation_split=validation_split_ks)"]},{"cell_type":"markdown","metadata":{"id":"-F8E079L0hzR"},"source":["Step 5: Evaluating the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-gpZZ7eC0sOY"},"outputs":[],"source":["score = model_ks.evaluate(x_test, y_test, verbose=0)\n","print(\"Test loss:\", score[0])\n","print(\"Test accuracy:\", score[1])"]},{"cell_type":"markdown","source":["## Tensorflow LeNet-5 Architecture (CNN)"],"metadata":{"id":"N4vbfgYCv_zQ"}},{"cell_type":"markdown","source":["Step 1: Getting, Splitting and Loading the Data"],"metadata":{"id":"5sFr66D-wNvl"}},{"cell_type":"code","source":["# Model / data parameters\n","num_classes = 10\n","input_shape = (28, 28, 1)\n","learn_rate = 0.005\n","epochs_tf = 10\n","batch_size_tf = 60000\n","\n","#TensorFlow - Getting and Splitting the Dataset\n","fashion_mnist = keras.datasets.fashion_mnist\n","#TensorFlow - Loading the Data\n","(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"],"metadata":{"id":"h5E9SHRawI1l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Step 2: Prepare the Data"],"metadata":{"id":"DEYNc1TbwOh9"}},{"cell_type":"code","source":["# Scale images to the [0, 1] range\n","x_train = x_train.astype(\"float32\") / 255\n","x_test = x_test.astype(\"float32\") / 255\n","\n","# Make sure images have shape (28, 28, 1)\n","x_train = np.expand_dims(x_train, -1)\n","x_test = np.expand_dims(x_test, -1)\n","\n","print(\"x_train shape:\", x_train.shape)\n","print(x_train.shape[0], \"train samples\")\n","print(x_test.shape[0], \"test samples\")\n","\n","# convert class vectors to binary class matrices\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yY2eqdsgwYd2","executionInfo":{"status":"ok","timestamp":1717691096619,"user_tz":-120,"elapsed":196,"user":{"displayName":"Fabio M","userId":"09614366475601969276"}},"outputId":"0c821021-3086-4fb5-95e2-adfa98427c3d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x_train shape: (60000, 28, 28, 1)\n","60000 train samples\n","10000 test samples\n"]}]},{"cell_type":"markdown","source":["Step 3: Building the Model"],"metadata":{"id":"OvLpVIzVwT_1"}},{"cell_type":"code","source":["class Model_tf(tf.Module):\n","    def __init__(self, name=\"LeNet_model_tf\"):\n","        super().__init__(name=name)\n","\n","        # Variables definition\n","        self.conv1 = tf.Variable(tf.random.normal([5, 5, input_shape[-1], 6]), name='conv1')\n","        self.conv2 = tf.Variable(tf.random.normal([5, 5, 6, 16]), name='conv2')\n","        self.fc1 = tf.Variable(tf.random.normal([784, 120]), name='fc1')\n","        self.fc2 = tf.Variable(tf.random.normal([120, 84]), name='fc2')\n","        self.fc3 = tf.Variable(tf.random.normal([84, num_classes]), name='fc3')\n","\n","        self.bias1 = tf.Variable(tf.zeros([6]), name='bias1')\n","        self.bias2 = tf.Variable(tf.zeros([16]), name='bias2')\n","        self.bias3 = tf.Variable(tf.zeros([120]), name='bias3')\n","        self.bias4 = tf.Variable(tf.zeros([84]), name='bias4')\n","        self.bias5 = tf.Variable(tf.zeros([num_classes]), name='bias5')\n","\n","    @tf.function\n","    def __call__(self, x):\n","        # 1st layer: Convolution\n","        x = tf.nn.conv2d(x, self.conv1, strides=[1, 1, 1, 1], padding='SAME')\n","        x = tf.nn.bias_add(x, self.bias1)\n","        x = tf.nn.relu(x)\n","        # 1st layer: Pooling\n","        x = tf.nn.avg_pool2d(x, ksize=2, strides=2, padding='SAME')\n","\n","        # 2nd layer: Convolution\n","        x = tf.nn.conv2d(x, self.conv2, strides=[1, 1, 1, 1], padding='SAME')\n","        x = tf.nn.bias_add(x, self.bias2)\n","        x = tf.nn.relu(x)\n","        # 2nd layer: Pooling\n","        x = tf.nn.avg_pool2d(x, ksize=2, strides=2, padding='SAME')\n","\n","        # flattening\n","        x = tf.reshape(x, [-1, 784])\n","\n","        # 3rd layer: Dense\n","        x = tf.nn.relu(tf.add(tf.matmul(x, self.fc1), self.bias3))\n","        # 4th layer: Dense\n","        x = tf.nn.relu(tf.add(tf.matmul(x, self.fc2), self.bias4))\n","        # 5th layer: Dense\n","        x = tf.add(tf.matmul(x, self.fc3), self.bias5)\n","        return tf.nn.softmax(x)\n","\n","\n","model_tf = Model_tf(name=\"LeNet5_model_tf\")\n"],"metadata":{"id":"2Qdxww4lwYNe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def loss_fn(y_true, y_pred):\n","  return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred))\n","\n","optimizer = tf.optimizers.Adam()"],"metadata":{"id":"ncOkw4MUN1FH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Step 4: Training the Model"],"metadata":{"id":"JQ8jai_AwVLt"}},{"cell_type":"code","source":["@tf.function\n","def train_step(model, X, y):\n","    with tf.GradientTape(persistent=True) as tape:\n","        y_model = model(X)\n","        loss = loss_fn(y, y_model)\n","    gradients = tape.gradient(loss, model.trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","    return loss\n","\n","def test_step(model, X, y, step=None):\n","    correct_predictions = tf.equal(y_test, model_tf(x_test))\n","    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32)) # Recast the Boolean as float32 first. Then calculate the mean.\n","    accuracy_value = accuracy.numpy()\n","    print(\"Epoch = {}   ,   Accuracy = {:5.3f}\".format(i, accuracy_value))\n","\n","\n","# Training.\n","for i in range(epochs_tf):\n","    idx_rnd = np.random.choice(range(x_train.shape[0]), batch_size_tf, replace=False)                          # Random sampling w/o replacement for the batch indices.\n","    batch_x = x_train[idx_rnd, :] # Sample a batch!\n","    batch_y = y_train[idx_rnd]\n","    train_step(model_tf, batch_x, batch_y)\n","\n","    if i%10 == 0:\n","      test_step(model_tf, batch_x, batch_y, i)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KG7dQHTxwX8e","executionInfo":{"status":"ok","timestamp":1717691444030,"user_tz":-120,"elapsed":26625,"user":{"displayName":"Fabio M","userId":"09614366475601969276"}},"outputId":"346178e7-212f-42b0-e09d-b485bdd68cc7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Step = 0   ,   Accuracy = 0.827\n"]}]},{"cell_type":"markdown","source":["Step 5: Evaluating the Model"],"metadata":{"id":"XFgHwuqnwWfW"}},{"cell_type":"code","source":["test_step(model_tf, batch_x, batch_y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DeM6quLKwXll","executionInfo":{"status":"ok","timestamp":1717691449562,"user_tz":-120,"elapsed":220,"user":{"displayName":"Fabio M","userId":"09614366475601969276"}},"outputId":"9620aa38-c5d6-481e-aaa8-76afcfd23177"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Step = 9   ,   Accuracy = 0.831\n"]}]},{"cell_type":"markdown","source":["## Jax-Flax LeNet-5 Architecture (CNN)\n","\n","---\n","\n","reference:\n","https://github.com/8bitmp3/JAX-Flax-Tutorial-Image-Classification-with-Linen"],"metadata":{"id":"buWkXQqSNtFz"}},{"cell_type":"markdown","source":["!pip install --upgrade -q pip jax jaxlib flax optax tensorflow-datasets"],"metadata":{"id":"lmNZ50FvgnPw"}},{"cell_type":"markdown","source":["Step 1: Getting, Splitting and Loading the Data"],"metadata":{"id":"f08bKrnpNyMb"}},{"cell_type":"code","source":["import jax\n","import jax.numpy as jnp               # JAX NumPy\n","\n","from flax import linen as nn          # The Linen API\n","from flax.training import train_state\n","import optax                          # The Optax gradient processing and optimization library\n","\n","import numpy as np                    # Ordinary NumPy\n","import tensorflow_datasets as tfds    # TFDS for MNIST"],"metadata":{"id":"urCuLAQ6NxqC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YXM12h7TUlIP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Step 2: Prepare the Data"],"metadata":{"id":"618TbyppOKB0"}},{"cell_type":"code","source":[],"metadata":{"id":"Ac_xifg7OPN6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Step 3: Building the Model"],"metadata":{"id":"eaUVOOEXOLUS"}},{"cell_type":"code","source":[],"metadata":{"id":"kFWpQwDNOPi6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Step 4: Training the Model"],"metadata":{"id":"BEXNZGM-OMAK"}},{"cell_type":"code","source":[],"metadata":{"id":"upcRDfNVOP4i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Step 5: Evaluating the Model"],"metadata":{"id":"dU-F-A3_OMtK"}},{"cell_type":"code","source":[],"metadata":{"id":"3ZUOazW5OQXa"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"17d1PWgH_pVube_ieHqlr64Lk-PaveGYp","timestamp":1715423387479}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Exercise #0706"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Convolutional Neural Network (grayscale images):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data            # MNIST hand written digits data!\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Download the MNIST data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verbosity_saved = tf.logging.get_verbosity()                        # Save the current verbosity lebel if needed.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)                            # Set the verbosity lebel high so that most warnings are ignored. \n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)         # Download the data.\n",
    "type(mnist)                                                           # Check the type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Take a look at the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data X shape: {}\".format((mnist.train.images).shape))\n",
    "print(\"Training data y shape: {}\".format((mnist.train.labels).shape))\n",
    "print(\"Training data cases: {}\".format(mnist.train.num_examples))\n",
    "print(\"\\n\")\n",
    "print(\"Testing data X shape: {}\".format((mnist.test.images).shape))\n",
    "print(\"Testing data y shape: {}\".format((mnist.test.labels).shape))\n",
    "print(\"Testing data cases: {}\".format(mnist.test.num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_image= 1                                                      # Image index. You can change it at will.\n",
    "a_single_image = mnist.train.images[i_image].reshape(28,28)     #  Reshape as a 2D array.\n",
    "plt.imshow(1-a_single_image, cmap='gist_gray')                  #  Display as grayscale image.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the minimum and maximum pixel value.\n",
    "# The data has been min-max-scaled already!\n",
    "print(\"MIN : {}\".format(a_single_image.min()))                 \n",
    "print(\"MAX : {}\".format(a_single_image.max())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Define the hyperparameters and placeholders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "n_epochs  = 10001\n",
    "learn_rate = 0.0001\n",
    "drop_prob = 0.5                                     # For the dropout layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ph = tf.placeholder(tf.float32, [None, 784])      # 'None' means any number of rows (observations or batch_size)\n",
    "y_ph = tf.placeholder(tf.float32,[None, 10])\n",
    "drop_prob_ph = tf.placeholder(tf.float32)           # The drop probability at the dropout layer is a hyperparameter.\n",
    "# X_ph must be reshaped in order to be fed into the conv2D function. \n",
    "# After reshaping, the shape becomes = [batch_size, in_height, in_width, in_channels].\n",
    "# -1 allows flexible batch_size.\n",
    "X_input_ph = tf.reshape(X_ph,[-1,28,28,1])             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Define the Variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration of the first convolution layer is as following:\n",
    "- Kernel height = 5.\n",
    "- Kernel width = 5.\n",
    "- In_chanels = 1 (gray scale). \n",
    "- Out_channels = 32 (number of feature maps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need Variables with the folllowing shapes:\n",
    "- Shape of the weight matrix = [kernel_height, kernel_width, in_channels, out_channels].\n",
    "- Shape of the bias = [out_channels]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables are defined according to the specifications mentioned above.\n",
    "W1 = tf.Variable(initial_value=tf.random_normal([5,5,1,32], mean=0, stddev=0.1))\n",
    "b1 = tf.Variable(initial_value=tf.fill([32], 0.1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration of the second convolution layer is as following: \n",
    "- Kernel height = 5.\n",
    "- Kernel width = 5.\n",
    "- In_chanels = 32 (out_channels from the previous convolution layer). \n",
    "- Out_channels = 64 (number of feature maps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we need Variables with the folllowing shapes:\n",
    "- Shape of the weight matrix = [kernel_height, kernel_width, in_channels, out_channels].\n",
    "- Shape of the bias = [out_channels]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables are defined according to the specifications mentioned above.\n",
    "W2 = tf.Variable(initial_value=tf.random_normal([5,5,32,64], mean=0, stddev=0.1))\n",
    "b2 = tf.Variable(initial_value=tf.fill([64], 0.1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the following considerations for the flattened fully connected layer:\n",
    "- We will apply convolution twice with padding and there will be no image size reduction. \n",
    "- We will also apply max pooling twice with stride = 2 (vertically and horizontally). \n",
    "- At each max pooling with stride = 2, the image size is halved. Thus, (28/2)/2 = 7 will be the size (vertical and horizontal) of the resulting final image.   \n",
    "- In the previous layer there were 64 output channels (feature maps). \n",
    "- Considering all these facts, there should be 7x7x64 = 3136 nodes in the flattened layer. \n",
    "- Finally, we will shrink the output from this layer to 1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables are defined according to the specifications mentioned above.\n",
    "W3 = tf.Variable(initial_value=tf.random_normal([3136,1024], mean=0, stddev=0.1))\n",
    "b3 = tf.Variable(initial_value=tf.fill([1024], 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the following considerations for the final output layer: \n",
    "- There are 1024 nodes to match with the output from the previous layer.\n",
    "- We should shrink the output once more because there are 10 different labels (digits 0~9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables are defined according to the specifications mentioned above.\n",
    "W4 = tf.Variable(initial_value=tf.random_normal([1024,10], mean=0, stddev=0.1))\n",
    "b4 = tf.Variable(initial_value=tf.fill([10], 0.1))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5. Define the deep learning model (CNN): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the arguments:\n",
    "- padding = 'SAME' to apply a padding. padding = 'VALID' to apply no padding. \n",
    "- ksize = [1, kernel_height, kernel_width, 1]\n",
    "- strides = [1, stride_vertical, stride_horizontal,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st Convolution layer.\n",
    "y1 = tf.nn.conv2d(X_input_ph, W1, strides=[1, 1, 1, 1], padding='SAME') + b1\n",
    "conv1 = tf.nn.relu(y1)                             # Apply the ReLu activation function. \n",
    "# 1st Pooling layer.\n",
    "pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd Convolution layer.\n",
    "y2 = tf.nn.conv2d(pool1, W2, strides=[1, 1, 1, 1], padding='SAME') + b2\n",
    "conv2 = tf.nn.relu(y2)                            # Apply the ReLu activation function. \n",
    "# 2nd Pooling layer.\n",
    "pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattened full layer.\n",
    "conv2_flattened = tf.reshape(pool2, [-1,3136])   # 7x7x64 = 3136.               \n",
    "y3 = tf.matmul(conv2_flattened, W3) + b3    \n",
    "full_layer = tf.nn.relu(y3)                      # Apply the ReLu activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout layer.\n",
    "dropout_layer = tf.nn.dropout(full_layer,rate = drop_prob_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layer.                 \n",
    "y_model = tf.matmul(dropout_layer, W4) + b4      # No activation function. Softmax at the output layer is optional. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6. Define the loss function and the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ph, logits=y_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate = learn_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7. Training and Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for i in range(n_epochs):\n",
    "            batch_X, batch_y = mnist.train.next_batch(batch_size)                                            # Sample a batch!\n",
    "            my_feed = {X_ph:batch_X, y_ph:batch_y, drop_prob_ph:drop_prob}\n",
    "            sess.run(train, feed_dict = my_feed)\n",
    "            if i % 500 == 0:            \n",
    "                correct_predictions = tf.equal(tf.argmax(y_ph, axis=1), tf.argmax(y_model, axis=1))          # In argmax(), axis=1 means horizontal direction.\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))                          # Recast the Boolean as float32 first. Then calculate the mean.\n",
    "                my_feed = {X_ph:mnist.test.images, y_ph:mnist.test.labels, drop_prob_ph:0.0}                 # No dropout for testing.\n",
    "                accuracy_value = sess.run(accuracy, feed_dict = my_feed)                              \n",
    "                print(\"Step = {}   ,   Accuracy = {:5.3f} \\n\".format(i, accuracy_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
